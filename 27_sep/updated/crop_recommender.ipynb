{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d075894e",
   "metadata": {},
   "source": [
    "\n",
    "# Crop Recommender System — End‑to‑End EDA & Data Readiness Notebook\n",
    "\n",
    "**Dataset**: Kaggle “Crop Recommendation Dataset” (N, P, K, temperature, pH, rainfall, label)  \n",
    "**Goal**: Build a clean, reusable pipeline that covers:\n",
    "- Loading & Sampling, removing unnecessary column (Read source files)\n",
    "- Data Preprocessing (type correction, time binning, missing values, outliers, encoding)\n",
    "- EDA (correlation analysis, visualizations, feature importance)\n",
    "- Feature Engineering (domain‑aware bins/placeholders)\n",
    "- Data Readiness for Modeling (train/val/test split)\n",
    "\n",
    "> ⚙️ **How to use**: Set `DATA_PATH` to your local CSV and run the notebook top‑down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e4118a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Config\n",
    "# Set this to your local path for the Kaggle CSV (e.g., 'crop_recommendation.csv')\n",
    "#DATA_PATH = r\"D:\\Study\\BITS\\code\\src\\archive (6)\\Crop Recommendation dataset\\Train Dataset.csv\"  # <-- change me after download\n",
    "DATA_PATH = \"./Downloads/Train Dataset.csv\" \n",
    "RANDOM_STATE = 42\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafaf53",
   "metadata": {},
   "source": [
    "## Loading and Sampling — Read source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e074e9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "CSV not found at C:\\Users\\himas\\Downloads\\archive\\updated\\Downloads\\Train Dataset.csv. Please update DATA_PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m \u001b[43mload_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# --- Canonicalize headers (idempotent)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcanon\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36mload_csv\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      3\u001b[0m p \u001b[38;5;241m=\u001b[39m Path(path)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please update DATA_PATH.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m df_ \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(p)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: CSV not found at C:\\Users\\himas\\Downloads\\archive\\updated\\Downloads\\Train Dataset.csv. Please update DATA_PATH."
     ]
    }
   ],
   "source": [
    "# Helper: safe load with friendly error\n",
    "def load_csv(path: str) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"CSV not found at {p.resolve()}. Please update DATA_PATH.\")\n",
    "    df_ = pd.read_csv(p)\n",
    "    return df_\n",
    "\n",
    "# Load\n",
    "df_raw = load_csv(DATA_PATH)\n",
    "\n",
    "# --- Canonicalize headers (idempotent)\n",
    "def canon(name: str) -> str:\n",
    "    return (\n",
    "        str(name)\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace('\\n',' ')\n",
    "        .replace('\\r',' ')\n",
    "        .replace('\\t',' ')\n",
    "        .replace('  ', ' ')\n",
    "        .replace(' ', '_')\n",
    "    )\n",
    "\n",
    "df_raw = df_raw.rename(columns={c: canon(c) for c in df_raw.columns})\n",
    "print(\"Canonicalized columns:\", list(df_raw.columns))\n",
    "\n",
    "# --- Try to detect target column if not already set\n",
    "possible_targets = ['label','crop','target','class','crop_label','cropname','crop_name']\n",
    "target_col = next((c for c in possible_targets if c in df_raw.columns), None)\n",
    "if target_col is None:\n",
    "    obj_cands = [c for c in df_raw.columns if df_raw[c].dtype == 'object']\n",
    "    if obj_cands:\n",
    "        ranked = sorted(obj_cands, key=lambda c: df_raw[c].nunique())\n",
    "        for c in ranked:\n",
    "            if df_raw[c].nunique() <= 100:\n",
    "                target_col = c\n",
    "                break\n",
    "if target_col is None:\n",
    "    raise RuntimeError(\"Could not detect the target column. Set `target_col` manually.\")\n",
    "\n",
    "print(\"Detected target column:\", target_col)\n",
    "\n",
    "\n",
    "#dropping column: 0 Unnamed , which is sequence number and doesn't have significance in crop recommendation\n",
    "df_raw = df_raw.drop(columns=[\"unnamed:_0\"])\n",
    "\n",
    "print(\"After dropping column, canonicalized columns:\", list(df_raw.columns))\n",
    "\n",
    "# Optional: rename the target to 'label' for consistency (rest of notebook uses target_col variable anyway)\n",
    "# df_raw = df_raw.rename(columns={target_col: 'label'})\n",
    "# target_col = 'label'\n",
    "\n",
    "# Stratified sample for quick experiments (set FRACTION < 1.0 to sample)\n",
    "FRACTION = 1.0  # e.g., 0.3 for a 30% sample\n",
    "if FRACTION < 1.0:\n",
    "    df_sampled = (df_raw\n",
    "                  .groupby(target_col, group_keys=False, sort=False)\n",
    "                  .apply(lambda x: x.sample(frac=FRACTION, random_state=RANDOM_STATE))\n",
    "                  .reset_index(drop=True))\n",
    "else:\n",
    "    df_sampled = df_raw.copy()\n",
    "\n",
    "print('Sampled shape:', df_sampled.shape)\n",
    "display(df_sampled[target_col].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28e8ba-a1b8-4c86-bb5f-ea7e8f150238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sampled.copy()\n",
    "\n",
    "\n",
    "# Build numeric list from known names if present; otherwise infer numeric\n",
    "#common_numeric = ['n','p','k','temperature','humidity','ph','rainfall']\n",
    "common_numeric = ['n','p','k','temperature','ph','rainfall']\n",
    "expected_numeric = [c for c in common_numeric if c in df.columns]\n",
    "\n",
    "# If some numeric columns are named differently, infer numeric by dtype as a fallback\n",
    "if not expected_numeric:\n",
    "    expected_numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != target_col]\n",
    "\n",
    "# Coerce numerics (bad parses -> NaN, handled later by imputers)\n",
    "for col in expected_numeric:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Time column detection (most Kaggle versions have no time column; this is safe if absent)\n",
    "candidate_time_cols = [c for c in df.columns if 'date' in c or c in ('time','timestamp','datetime','month')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d9d79d",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198c4e5",
   "metadata": {},
   "source": [
    "### Data type correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c84775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sampled.copy()\n",
    "\n",
    "\n",
    "# Canonical alias map for the Kaggle crop dataset\n",
    "alias_map = {\n",
    "    'n': ['n', 'nitrogen'],\n",
    "    'p': ['p', 'phosphorus'],\n",
    "    'k': ['k', 'potassium'],\n",
    "    'temperature': ['temperature', 'temp', 'temperature_c', 'avg_temp', 'average_temperature'],\n",
    "    #'humidity': ['humidity', 'relative_humidity', 'rel_humidity'],\n",
    "    'ph': ['ph', 'soil_ph'],\n",
    "    'rainfall': ['rainfall', 'rain_fall', 'annual_rainfall', 'rain', 'rainfall_mm']\n",
    "}\n",
    "\n",
    "# Build a mapping from canonical name -> actual present column in df (first match wins)\n",
    "present_map = {}\n",
    "cols_lower = {canon(c): c for c in df.columns}  # canon->original\n",
    "\n",
    "for canon_key, candidates in alias_map.items():\n",
    "    found = None\n",
    "    for cand in candidates:\n",
    "        ckey = canon(cand)\n",
    "        if ckey in cols_lower:\n",
    "            found = cols_lower[ckey]\n",
    "            break\n",
    "    if found:\n",
    "        present_map[canon_key] = found\n",
    "\n",
    "# Final list of numeric columns we expect (those actually present)\n",
    "expected_numeric = list(present_map.values())\n",
    "\n",
    "# If none matched (unusual), fall back to dtype-based inference (exclude the target)\n",
    "if not expected_numeric:\n",
    "    expected_numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != target_col]\n",
    "\n",
    "# Coerce numerics (bad parses -> NaN; handled later by imputers)\n",
    "for col in expected_numeric:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Time column detection (no-op for typical Kaggle file)\n",
    "candidate_time_cols = [c for c in df.columns if any(k in c for k in ['date','time','timestamp','datetime','month'])]\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1be25f",
   "metadata": {},
   "source": [
    "### Classify time data into categories or bins (if time columns exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1716620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def month_to_season(m):\n",
    "    # Simple Northern Hemisphere mapping (adjust for locale),\n",
    "    # returns one of ['Winter','Spring','Summer','Autumn'] or None\n",
    "    if pd.isna(m): return None\n",
    "    m = int(m)\n",
    "    if m in (12,1,2):  return 'Winter'\n",
    "    if m in (3,4,5):   return 'Spring'\n",
    "    if m in (6,7,8):   return 'Summer'\n",
    "    if m in (9,10,11): return 'Autumn'\n",
    "    return None\n",
    "\n",
    "added_cols = []\n",
    "for c in candidate_time_cols:\n",
    "    if np.issubdtype(df[c].dtype, np.datetime64):\n",
    "        df[c + '_month']  = df[c].dt.month\n",
    "        df[c + '_season'] = df[c + '_month'].apply(month_to_season)\n",
    "        added_cols.extend([c + '_month', c + '_season'])\n",
    "    elif df[c].dtype == 'int64' or df[c].dtype == 'float64':\n",
    "        # If it's numeric month already\n",
    "        df[c + '_season'] = df[c].apply(month_to_season)\n",
    "        added_cols.append(c + '_season')\n",
    "\n",
    "print('Time-derived columns:', added_cols)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add77ade",
   "metadata": {},
   "source": [
    "### Handling missing values — audit & plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac87474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def missing_report(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    miss = frame.isna().sum().sort_values(ascending=False)\n",
    "    pct  = (miss / len(frame)).round(4)\n",
    "    out  = pd.DataFrame({'missing': miss, 'pct': pct})\n",
    "    return out[out['missing'] > 0]\n",
    "\n",
    "missing_report(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6238b14",
   "metadata": {},
   "source": [
    "### Outlier detection (Z-score & IQR) — non-destructive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb8c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_summary_numeric(frame: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        if c not in frame.columns:\n",
    "            # Skip silently if alias wasn't found\n",
    "            continue\n",
    "        s = pd.to_numeric(frame[c], errors='coerce').dropna().astype(float)\n",
    "        if s.empty:\n",
    "            continue\n",
    "        q1, q3 = np.percentile(s, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        low, high = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "        iqr_count = int(((s < low) | (s > high)).sum())\n",
    "\n",
    "        std = s.std(ddof=0)\n",
    "        std = std if std and not np.isnan(std) else 1.0\n",
    "        z_count = int((np.abs((s - s.mean()) / std) > 3.0).sum())\n",
    "\n",
    "        rows.append({'col': c, 'iqr_outliers': iqr_count, 'z>3_outliers': z_count})\n",
    "    return pd.DataFrame(rows).sort_values('iqr_outliers', ascending=False)\n",
    "\n",
    "outlier_summary_numeric(df, expected_numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7fe2a",
   "metadata": {},
   "source": [
    "### Encoding — label encode target, one-hot any categorical features (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expected target\n",
    "expected_target = \"crop\"\n",
    "# Identify columns\n",
    "target_col = expected_target if expected_target in df.columns else None\n",
    "feature_cols = [c for c in df.columns if c != target_col]\n",
    "\n",
    "# Categorical feature candidates (excluding target)\n",
    "cat_cols = [c for c in feature_cols if df[c].dtype == 'object' or str(df[c].dtype).startswith('category')]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "print('Target:', target_col)\n",
    "print('Numeric features:', num_cols)\n",
    "print('Categorical features:', cat_cols[:10], '(showing up to 10)')\n",
    "\n",
    "# Label encode target\n",
    "le = None\n",
    "if target_col:\n",
    "    le = LabelEncoder()\n",
    "    df[target_col] = le.fit_transform(df[target_col])\n",
    "    classes_ = list(le.classes_)\n",
    "    print('Target classes:', classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068382df",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4df6df",
   "metadata": {},
   "source": [
    "### Correlation Analysis (numeric only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = df[num_cols].copy()\n",
    "corr = num_df.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "im = plt.imshow(corr, interpolation='nearest', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xticks(range(len(num_df.columns)), num_df.columns, rotation=45, ha='right')\n",
    "plt.yticks(range(len(num_df.columns)), num_df.columns)\n",
    "plt.colorbar(im)\n",
    "\n",
    "# Add numbers inside the cells\n",
    "for i in range(len(corr.columns)):\n",
    "    for j in range(len(corr.columns)):\n",
    "        text = f\"{corr.iloc[i, j]:.2f}\"\n",
    "        plt.text(j, i, text,\n",
    "                 ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f5656",
   "metadata": {},
   "source": [
    "### Visualizations: distributions & boxplots (numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1673f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histograms\n",
    "for c in num_cols:\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.hist(df[c].dropna(), bins=30)\n",
    "    plt.title(f'Distribution: {c}')\n",
    "    plt.xlabel(c); plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Boxplots\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.boxplot([df[c].dropna() for c in num_cols], labels=num_cols, vert=True, showmeans=True)\n",
    "plt.title('Boxplots (Numeric Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abc259",
   "metadata": {},
   "source": [
    "### Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure target_col is defined (re-detect if not)\n",
    "if 'target_col' not in locals() or target_col is None:\n",
    "    possible_targets = ['label','crop','target','class','crop_label','cropname','crop_name']\n",
    "    target_col = next((c for c in possible_targets if c in df.columns), None)\n",
    "\n",
    "if target_col is not None:\n",
    "    vc = df[target_col].value_counts().sort_index()\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.bar(range(len(vc)), vc.values, tick_label=vc.index)\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xlabel('Class'); plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    display(vc.head())\n",
    "else:\n",
    "    print('Could not detect target column. Please set `target_col` manually, e.g.:')\n",
    "    print(\"target_col = 'crop'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699518e",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ec75a",
   "metadata": {},
   "source": [
    "\n",
    "Below are simple, domain‑aware starters you can toggle on/off:\n",
    "- **pH bins** (acidic/neutral/alkaline)\n",
    "- **Rainfall bins** (low/medium/high)\n",
    "- **Temperature bins** (cool/warm/hot)\n",
    "- **Season** (if time available)\n",
    "\n",
    "These engineered columns are optional and can be dropped later if they don’t help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_domain_bins(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = frame.copy()\n",
    "\n",
    "    if 'ph' in g.columns:\n",
    "        g['ph_bin'] = pd.cut(g['ph'], bins=[-np.inf, 6.0, 7.5, np.inf], labels=['acidic','neutral','alkaline'])\n",
    "    if 'rainfall' in g.columns:\n",
    "        # bins roughly tuned to the Kaggle dataset distribution\n",
    "        q = g['rainfall'].quantile([0.33, 0.66])\n",
    "        g['rainfall_bin'] = pd.cut(g['rainfall'], bins=[-np.inf, q.iloc[0], q.iloc[1], np.inf],\n",
    "                                   labels=['low','medium','high'])\n",
    "    if 'temperature' in g.columns:\n",
    "        q = g['temperature'].quantile([0.33, 0.66])\n",
    "        g['temp_bin'] = pd.cut(g['temperature'], bins=[-np.inf, q.iloc[0], q.iloc[1], np.inf],\n",
    "                               labels=['cool','warm','hot'])\n",
    "    return g\n",
    "\n",
    "df_fe = add_domain_bins(df)\n",
    "df_fe.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa4ffa",
   "metadata": {},
   "source": [
    "## Data Readiness for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac37e11",
   "metadata": {},
   "source": [
    "### Train/Validation/Test split (stratified where possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if target_col is None:\n",
    "    raise RuntimeError(\"target_col is not set. Please detect or set it before splitting.\")\n",
    "\n",
    "y_raw = df_fe[target_col]\n",
    "\n",
    "# Ensure numeric labels (consistent across all splits)\n",
    "le = LabelEncoder()\n",
    "y_all = le.fit_transform(y_raw.astype(str))\n",
    "class_names = list(le.classes_)  # keep for later reports/plots\n",
    "\n",
    "X_all = df_fe.drop(columns=[target_col])\n",
    "\n",
    "# 70/15/15 stratified split on encoded labels\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    X_all, y_all, test_size=0.30, random_state=RANDOM_STATE, stratify=y_all\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.50, random_state=RANDOM_STATE, stratify=y_tmp\n",
    ")\n",
    "\n",
    "print('Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "\n",
    "# Safe class distribution printouts\n",
    "train_counts = np.bincount(y_train, minlength=len(class_names))\n",
    "val_counts   = np.bincount(y_val,   minlength=len(class_names))\n",
    "test_counts  = np.bincount(y_test,  minlength=len(class_names))\n",
    "\n",
    "print('Target distribution (train):', train_counts[:10], '...')\n",
    "print('Target distribution (val):  ', val_counts[:10],  '...')\n",
    "print('Target distribution (test): ', test_counts[:10], '...')\n",
    "\n",
    "# Optional: pretty table of counts per class\n",
    "import pandas as pd\n",
    "dist = pd.DataFrame({\n",
    "    'class': class_names,\n",
    "    'train': train_counts,\n",
    "    'val':   val_counts,\n",
    "    'test':  test_counts\n",
    "}).sort_values('class').reset_index(drop=True)\n",
    "display(dist.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8dbdd",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline (impute → encode → scale numeric (optional))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7369cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numeric pipeline\n",
    "num_tf = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # Uncomment the next line if you plan to use distance-based models\n",
    "    # ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "cat_tf = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_tf, num_cols),\n",
    "        ('cat', cat_tf, cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Example baseline model (can be swapped later)\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[('prep', preprocess), ('model', clf)])\n",
    "pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca539a45",
   "metadata": {},
   "source": [
    "### Fit baseline & evaluate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_val_pred = pipe.predict(X_val)\n",
    "\n",
    "print('Validation classification report:')\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))\n",
    "\n",
    "print('Confusion matrix (top-left 10x10 block if large):')\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(cm[:10,:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a1b272",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8b40c",
   "metadata": {},
   "source": [
    "### Model-based importances (RandomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ct_feature_names_fitted(ct, input_features):\n",
    "    \"\"\"\n",
    "    Return output feature names from a *fitted* ColumnTransformer.\n",
    "    Uses ct.get_feature_names_out when available, otherwise unwraps fitted inner estimators.\n",
    "    \"\"\"\n",
    "    if not hasattr(ct, \"transformers_\"):\n",
    "        raise NotFittedError(\"ColumnTransformer is not fitted yet. Fit your pipeline before calling this.\")\n",
    "\n",
    "    # 1) Best path (sklearn >= 1.0+)\n",
    "    try:\n",
    "        return ct.get_feature_names_out(input_features)\n",
    "    except Exception:\n",
    "        pass  # fall back to manual path below\n",
    "\n",
    "    # 2) Manual path (robust across versions)\n",
    "    output_features = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        # skip explicitly dropped groups\n",
    "        if name == \"remainder\" and trans == \"drop\":\n",
    "            continue\n",
    "\n",
    "        # fetch the *fitted* transformer by name (important!)\n",
    "        fitted_trans = ct.named_transformers_.get(name, trans)\n",
    "\n",
    "        # passthrough: keep original column names\n",
    "        if fitted_trans == \"passthrough\" or trans == \"passthrough\":\n",
    "            output_features.extend(list(cols))\n",
    "            continue\n",
    "\n",
    "        # unwrap pipelines to their last step\n",
    "        last = fitted_trans\n",
    "        if hasattr(fitted_trans, \"steps\"):\n",
    "            last = fitted_trans.steps[-1][1]\n",
    "\n",
    "        # try to get names from the last step if it exposes them\n",
    "        if hasattr(last, \"get_feature_names_out\"):\n",
    "            try:\n",
    "                fn = last.get_feature_names_out(cols)\n",
    "            except NotFittedError:\n",
    "                raise  # surface a clear error if somehow not fitted\n",
    "            except TypeError:\n",
    "                fn = np.asarray(cols, dtype=object)\n",
    "        else:\n",
    "            fn = np.asarray(cols, dtype=object)\n",
    "\n",
    "        output_features.extend(fn.tolist())\n",
    "\n",
    "    return np.asarray(output_features, dtype=object)\n",
    "\n",
    "# ---- Fit, get names, and plot importances ----\n",
    "\n",
    "print(\"columns =\", X_train.columns)\n",
    "\n",
    "# Make sure the *whole* pipeline is fitted\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "prep = pipe.named_steps[\"prep\"]\n",
    "model = pipe.named_steps[\"model\"]\n",
    "\n",
    "# robust feature-name extraction\n",
    "feature_names = get_ct_feature_names_fitted(prep, X_train.columns)\n",
    "\n",
    "# Guard: ensure lengths match (in case of odd transformers)\n",
    "if hasattr(model, \"feature_importances_\"):\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    # If there is any mismatch, align to transformed shape\n",
    "    if len(feature_names) != len(importances):\n",
    "        Xt = prep.transform(X_train)\n",
    "        feature_names = np.array(feature_names)[:Xt.shape[1]]\n",
    "        importances = importances[:Xt.shape[1]]\n",
    "\n",
    "    imp = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "    imp = imp.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    display(imp.head(20))\n",
    "\n",
    "    top_k = 20\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(imp[\"feature\"].head(top_k)[::-1], imp[\"importance\"].head(top_k)[::-1])\n",
    "    plt.title(\"Top Feature Importances (RandomForest)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Model does not expose feature_importances_.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21f5ac",
   "metadata": {},
   "source": [
    "### Permutation importance (validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1322a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Run on the pipeline so permutation happens on raw columns\n",
    "r = permutation_importance(pipe, X_val, y_val,\n",
    "                           n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "perm_imp = (pd.DataFrame({\n",
    "    'feature': X_val.columns,\n",
    "    'importance_mean': r.importances_mean,\n",
    "    'importance_std':  r.importances_std\n",
    "})\n",
    ".sort_values('importance_mean', ascending=False)\n",
    ".reset_index(drop=True))\n",
    "\n",
    "display(perm_imp.head(20))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(perm_imp['feature'].head(20)[::-1], perm_imp['importance_mean'].head(20)[::-1])\n",
    "plt.title('Top Permutation Importances (Validation)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0b3d8",
   "metadata": {},
   "source": [
    "## Handling Missing Values for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe13c8",
   "metadata": {},
   "source": [
    "\n",
    "Missing value handling is baked into the preprocessing pipeline via `SimpleImputer`:\n",
    "- Numeric: median\n",
    "- Categorical: most frequent\n",
    "\n",
    "You can change strategies (e.g., `mean`, `constant`) in the pipeline if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f0e99",
   "metadata": {},
   "source": [
    "## Classify Time Data into Categories or Bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaee76d",
   "metadata": {},
   "source": [
    "\n",
    "If your data includes a date/time column, we derive `*_month` and `*_season`.  \n",
    "This Kaggle dataset does **not** include time, so the step is **no‑op** unless you add such a column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2035da",
   "metadata": {},
   "source": [
    "## Final Model & Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da49a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Refit on train+val, then evaluate on held-out test\n",
    "X_trval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trval = np.concatenate([y_train, y_val])\n",
    "\n",
    "pipe.fit(X_trval, y_trval)\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "print('Test classification report:')\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n",
    "\n",
    "# Save artifacts if needed\n",
    "out_dir = Path('artifacts')\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "# Save processed splits for future modeling\n",
    "pd.concat([X_train, pd.Series(y_train, name=target_col, index=X_train.index)], axis=1).to_csv(out_dir/'train.csv', index=False)\n",
    "pd.concat([X_val,   pd.Series(y_val,   name=target_col, index=X_val.index)],   axis=1).to_csv(out_dir/'val.csv', index=False)\n",
    "pd.concat([X_test,  pd.Series(y_test,  name=target_col, index=X_test.index)],  axis=1).to_csv(out_dir/'test.csv', index=False)\n",
    "\n",
    "print('Saved splits to', out_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53961d97",
   "metadata": {},
   "source": [
    "## Optional: Outlier filtering recipe (IQR capping / IsolationForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b993fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iqr_cap(frame: pd.DataFrame, cols: List[str], k: float = 1.5) -> pd.DataFrame:\n",
    "    g = frame.copy()\n",
    "    for c in cols:\n",
    "        s = g[c].astype(float)\n",
    "        q1, q3 = np.percentile(s.dropna(), [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        low, high = q1 - k*iqr, q3 + k*iqr\n",
    "        g[c] = s.clip(lower=low, upper=high)\n",
    "    return g\n",
    "\n",
    "# Example usage (commented out by default):\n",
    "df_capped = iqr_cap(df, expected_numeric, k=1.5)\n",
    "\n",
    "# IsolationForest-based removal (use cautiously for small datasets):\n",
    "iso = IsolationForest(random_state=RANDOM_STATE, contamination=0.03)\n",
    "mask = iso.fit_predict(df[expected_numeric]) == 1\n",
    "df_iso = df[mask].reset_index(drop=True)\n",
    "print('IsolationForest kept rows:', df_iso.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3490b",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('DataFrame head (post FE):')\n",
    "df_fe.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
